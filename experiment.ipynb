{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONNECT 4: RL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\venvs\\rtg\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Ayden\\dev\\rl_connect_four\\env.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ayden/dev/rl_connect_four/env.ipynb#ch0000020?line=9'>10</a>\u001b[0m model \u001b[39m=\u001b[39m DQN(\u001b[39m'\u001b[39m\u001b[39mMlpPolicy\u001b[39m\u001b[39m'\u001b[39m, env, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ayden/dev/rl_connect_four/env.ipynb#ch0000020?line=10'>11</a>\u001b[0m \u001b[39m# Train the agent\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Ayden/dev/rl_connect_four/env.ipynb#ch0000020?line=11'>12</a>\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(\u001b[39m2e5\u001b[39;49m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ayden/dev/rl_connect_four/env.ipynb#ch0000020?line=12'>13</a>\u001b[0m \u001b[39m# Save the agent\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ayden/dev/rl_connect_four/env.ipynb#ch0000020?line=13'>14</a>\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mconnect-4-stable_baselines3-dqn\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\venvs\\rtg\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:258\u001b[0m, in \u001b[0;36mDQN.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/dqn/dqn.py?line=244'>245</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/dqn/dqn.py?line=245'>246</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/dqn/dqn.py?line=246'>247</a>\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/dqn/dqn.py?line=254'>255</a>\u001b[0m     reset_num_timesteps: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/dqn/dqn.py?line=255'>256</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m OffPolicyAlgorithm:\n\u001b[1;32m--> <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/dqn/dqn.py?line=257'>258</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(DQN, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/dqn/dqn.py?line=258'>259</a>\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/dqn/dqn.py?line=259'>260</a>\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/dqn/dqn.py?line=260'>261</a>\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/dqn/dqn.py?line=261'>262</a>\u001b[0m         eval_env\u001b[39m=\u001b[39;49meval_env,\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/dqn/dqn.py?line=262'>263</a>\u001b[0m         eval_freq\u001b[39m=\u001b[39;49meval_freq,\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/dqn/dqn.py?line=263'>264</a>\u001b[0m         n_eval_episodes\u001b[39m=\u001b[39;49mn_eval_episodes,\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/dqn/dqn.py?line=264'>265</a>\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/dqn/dqn.py?line=265'>266</a>\u001b[0m         eval_log_path\u001b[39m=\u001b[39;49meval_log_path,\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/dqn/dqn.py?line=266'>267</a>\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/dqn/dqn.py?line=267'>268</a>\u001b[0m     )\n",
      "File \u001b[1;32mC:\\venvs\\rtg\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:347\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/off_policy_algorithm.py?line=343'>344</a>\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/off_policy_algorithm.py?line=345'>346</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[1;32m--> <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/off_policy_algorithm.py?line=346'>347</a>\u001b[0m     rollout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/off_policy_algorithm.py?line=347'>348</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv,\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/off_policy_algorithm.py?line=348'>349</a>\u001b[0m         train_freq\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_freq,\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/off_policy_algorithm.py?line=349'>350</a>\u001b[0m         action_noise\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_noise,\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/off_policy_algorithm.py?line=350'>351</a>\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/off_policy_algorithm.py?line=351'>352</a>\u001b[0m         learning_starts\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearning_starts,\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/off_policy_algorithm.py?line=352'>353</a>\u001b[0m         replay_buffer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplay_buffer,\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/off_policy_algorithm.py?line=353'>354</a>\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/off_policy_algorithm.py?line=354'>355</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/off_policy_algorithm.py?line=356'>357</a>\u001b[0m     \u001b[39mif\u001b[39;00m rollout\u001b[39m.\u001b[39mcontinue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/off_policy_algorithm.py?line=357'>358</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\venvs\\rtg\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:580\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/off_policy_algorithm.py?line=576'>577</a>\u001b[0m actions, buffer_actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[39m.\u001b[39mnum_envs)\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/off_policy_algorithm.py?line=578'>579</a>\u001b[0m \u001b[39m# Rescale and perform action\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/off_policy_algorithm.py?line=579'>580</a>\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(actions)\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/off_policy_algorithm.py?line=581'>582</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/off_policy_algorithm.py?line=582'>583</a>\u001b[0m num_collected_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mC:\\venvs\\rtg\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:162\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/vec_env/base_vec_env.py?line=154'>155</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/vec_env/base_vec_env.py?line=155'>156</a>\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/vec_env/base_vec_env.py?line=156'>157</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/vec_env/base_vec_env.py?line=157'>158</a>\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/vec_env/base_vec_env.py?line=158'>159</a>\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/vec_env/base_vec_env.py?line=159'>160</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/vec_env/base_vec_env.py?line=160'>161</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/vec_env/base_vec_env.py?line=161'>162</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[1;32mC:\\venvs\\rtg\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:43\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=40'>41</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=41'>42</a>\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m---> <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=42'>43</a>\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[0;32m     <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=43'>44</a>\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[0;32m     <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=44'>45</a>\u001b[0m         )\n\u001b[0;32m     <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=45'>46</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx]:\n\u001b[0;32m     <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=46'>47</a>\u001b[0m             \u001b[39m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=47'>48</a>\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx][\u001b[39m\"\u001b[39m\u001b[39mterminal_observation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m obs\n",
      "File \u001b[1;32mC:\\venvs\\rtg\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:90\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/monitor.py?line=87'>88</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[0;32m     <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/monitor.py?line=88'>89</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/monitor.py?line=89'>90</a>\u001b[0m observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/monitor.py?line=90'>91</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[0;32m     <a href='file:///c%3A/venvs/rtg/lib/site-packages/stable_baselines3/common/monitor.py?line=91'>92</a>\u001b[0m \u001b[39mif\u001b[39;00m done:\n",
      "File \u001b[1;32mc:\\Users\\Ayden\\dev\\rl_connect_four\\env.py:102\u001b[0m, in \u001b[0;36mConnect4Env.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Ayden/dev/rl_connect_four/env.py?line=98'>99</a>\u001b[0m reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/Ayden/dev/rl_connect_four/env.py?line=100'>101</a>\u001b[0m \u001b[39m# Take my action, player = 1\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Ayden/dev/rl_connect_four/env.py?line=101'>102</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute_action(action\u001b[39m=\u001b[39;49maction, player\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m    <a href='file:///c%3A/Users/Ayden/dev/rl_connect_four/env.py?line=103'>104</a>\u001b[0m done, winner \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_done() \u001b[39m# checks if game is over and who won\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Ayden/dev/rl_connect_four/env.py?line=105'>106</a>\u001b[0m \u001b[39m# If game not done, opponenent takes an action:\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ayden\\dev\\rl_connect_four\\env.py:85\u001b[0m, in \u001b[0;36mConnect4Env.execute_action\u001b[1;34m(self, action, player)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Ayden/dev/rl_connect_four/env.py?line=78'>79</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexecute_action\u001b[39m(\u001b[39mself\u001b[39m, action: \u001b[39mfloat\u001b[39m, player: \u001b[39mint\u001b[39m):\n\u001b[0;32m     <a href='file:///c%3A/Users/Ayden/dev/rl_connect_four/env.py?line=79'>80</a>\u001b[0m     \u001b[39m\"\"\"Executes an action in the games current state.\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Ayden/dev/rl_connect_four/env.py?line=80'>81</a>\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Ayden/dev/rl_connect_four/env.py?line=81'>82</a>\u001b[0m \u001b[39m        action (float): an action - which column to place a chip, continuous range (0,6)\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Ayden/dev/rl_connect_four/env.py?line=82'>83</a>\u001b[0m \u001b[39m        player (int): either 1 (the RL agent) or -1 (opponenent)\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Ayden/dev/rl_connect_four/env.py?line=83'>84</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/Ayden/dev/rl_connect_four/env.py?line=84'>85</a>\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mround\u001b[39m(action[\u001b[39m0\u001b[39;49m]) \u001b[39mif\u001b[39;00m DISCRET_ACTION_SPACE \u001b[39melse\u001b[39;00m action \u001b[39m# converts to integer\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Ayden/dev/rl_connect_four/env.py?line=85'>86</a>\u001b[0m     \u001b[39m# The action specifies the column, iterates from the bottom row to find a clear space\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Ayden/dev/rl_connect_four/env.py?line=86'>87</a>\u001b[0m     \u001b[39m# to insert a chip.\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Ayden/dev/rl_connect_four/env.py?line=87'>88</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "from env import Connect4Env\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "\n",
    "# Create environment\n",
    "env = Connect4Env()\n",
    "\n",
    "# Instantiate the agent\n",
    "model = DQN('MlpPolicy', env, verbose=1)\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(2e5))\n",
    "# Save the agent\n",
    "model.save(\"connect-4-stable_baselines3-dqn\")\n",
    "del model  # delete trained model to demonstrate loading\n",
    "\n",
    "# Load the trained agent\n",
    "# NOTE: if you have loading issue, you can pass `print_system_info=True`\n",
    "# to compare the system on which the model was trained vs the current one\n",
    "# model = DQN.load(\"dqn_lunar\", env=env, print_system_info=True)\n",
    "model = DQN.load(\"dqn_lunar\", env=env)\n",
    "\n",
    "# Evaluate the agent\n",
    "# NOTE: If you use wrappers with your environment that modify rewards,\n",
    "#       this will be reflected here. To evaluate with original rewards,\n",
    "#       wrap environment in a \"Monitor\" wrapper before other wrappers.\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "\n",
    "# Enjoy trained agent\n",
    "obs = env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rtg_kernel",
   "language": "python",
   "name": "rtg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
